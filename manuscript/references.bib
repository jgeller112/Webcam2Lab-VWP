@article{bramlettArtWranglingWorking2025,
  title = {The Art of Wrangling: {{Working}} with Web-Based Visual World Paradigm Eye-Tracking Data in Language Research},
  shorttitle = {The Art of Wrangling},
  author = {Bramlett, Adam A. and Wiener, Seth},
  year = 2025,
  month = aug,
  journal = {Linguistic Approaches to Bilingualism},
  volume = {15},
  number = {4},
  pages = {538--570},
  publisher = {John Benjamins},
  issn = {1879-9264, 1879-9272},
  doi = {10.1075/lab.23071.bra},
  urldate = {2025-10-20},
  abstract = {Abstract Web-based eye-tracking is more accessible than ever. Researchers can now carry out visual world paradigm studies remotely and access never before tested, multilingual populations via the internet all without the need for an expensive eye-tracker. Web-based eye-tracking, however, requires careful experimental design and extensive data wrangling skills. In this paper, we provide a framework for reproducible, open science visual world paradigm studies using online experiments. We provide step-by-step instructions to building a typical visual world paradigm psycholinguistics study, and walk the reader through a series of data wrangling steps needed to prepare the data for visualization and analysis using the open-source software environment, R. Importantly, we highlight the key decisions researchers need to make and report in order to reproduce an analysis. We demonstrate our approach by carrying out a single change replication of an in-person eye-tracking study by Porretta et al. (2020). We conclude with best practices and recommendations for researchers carrying out bi-/multilingualism web-based visual world paradigm studies.},
  langid = {english}
}

@misc{chengAppearancebasedGazeEstimation2024,
  title = {Appearance-Based {{Gaze Estimation With Deep Learning}}: {{A Review}} and {{Benchmark}}},
  shorttitle = {Appearance-Based {{Gaze Estimation With Deep Learning}}},
  author = {Cheng, Yihua and Wang, Haofei and Bao, Yiwei and Lu, Feng},
  year = 2024,
  month = apr,
  number = {arXiv:2104.12668},
  eprint = {2104.12668},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2104.12668},
  urldate = {2025-09-22},
  abstract = {Human gaze provides valuable information on human focus and intentions, making it a crucial area of research. Recently, deep learning has revolutionized appearance-based gaze estimation. However, due to the unique features of gaze estimation research, such as the unfair comparison between 2D gaze positions and 3D gaze vectors and the different pre-processing and post-processing methods, there is a lack of a definitive guideline for developing deep learning-based gaze estimation algorithms. In this paper, we present a systematic review of the appearance-based gaze estimation methods using deep learning. Firstly, we survey the existing gaze estimation algorithms along the typical gaze estimation pipeline: deep feature extraction, deep learning model design, personal calibration and platforms. Secondly, to fairly compare the performance of different approaches, we summarize the data pre-processing and post-processing methods, including face/eye detection, data rectification, 2D/3D gaze conversion and gaze origin conversion. Finally, we set up a comprehensive benchmark for deep learning-based gaze estimation. We characterize all the public datasets and provide the source code of typical gaze estimation algorithms. This paper serves not only as a reference to develop deep learning-based gaze estimation methods, but also a guideline for future gaze estimation research. The project web page can be found at https://phi-ai.buaa.edu.cn/Gazehub.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/jasongeller/Zotero/storage/HEU4KKYE/Cheng et al. - 2024 - Appearance-based Gaze Estimation With Deep Learning A Review and Benchmark.pdf}
}

@article{gellerLanguageBordersStepbystep2025f,
  title = {Language without Borders: {{A}} Step-by-Step Guide to Analyzing Webcam Eye-Tracking Data for {{L2}} Research},
  shorttitle = {Language without Borders},
  author = {Geller, Jason and Prystauka, Yanina and Colby, Sarah E. and Drouin, Julia R.},
  year = 2025,
  month = dec,
  journal = {Research Methods in Applied Linguistics},
  volume = {4},
  number = {3},
  pages = {100226},
  issn = {2772-7661},
  doi = {10.1016/j.rmal.2025.100226},
  urldate = {2025-09-22},
  abstract = {Eye-tracking has become a valuable tool for studying cognitive processes in second language acquisition and bilingualism (Godfroid et al., 2024). While research-grade infrared eye-trackers are commonly used, several factors limit their widespread adoption. Recently, consumer-based webcam eye-tracking has emerged as an attractive alternative, requiring only a personal webcam and internet access. However, webcam-based eye-tracking introduces unique design and preprocessing challenges that must be addressed to ensure valid results. To help researchers navigate these challenges, we developed a comprehensive tutorial focused on visual world webcam eye-tracking for second language research. This guide covers key preprocessing steps---from reading in raw data to visualization and analysis---highlighting the open-source R package webgazeR (Geller, 2025), freely available at: https://github.com/jgeller112/webgazer. To demonstrate these steps, we analyze data collected via the Gorilla platform (Anwyl-Irvine et al., 2020) using a single-word Spanish visual world paradigm (VWP), showcasing evidence of competition both within and between Spanish and English. This tutorial aims to empower researchers by providing a step-by-step guide to successfully conduct webcam-based visual world eye-tracking studies. To follow along, please download the complete manuscript, code, and data from: https://github.com/jgeller112/L2\_VWP\_Webcam.},
  keywords = {Gorilla,L2 processing,R,Spoken word recognition,Tutorial,VWP,Webcam eye-tracking}
}

@article{jamesWhatParadigmsCan2025,
  title = {What {{Paradigms Can Webcam Eye-Tracking Be Used For}}? {{Attempted Replications}} of {{Five Cognitive Science Experiments}}},
  shorttitle = {What {{Paradigms Can Webcam Eye-Tracking Be Used For}}?},
  author = {James, Ariel N. and Ryskin, Rachel and Hartshorne, Joshua K. and Backs, Haylee and Bala, Nandeeta and {Barcenas-Meade}, Laila and Bhattarai, Samata and Charles, Tessa and Copoulos, Gerasimos and Coss, Claire and Eisert, Alexander and Furuhashi, Elena and Ginell, Keara and {Guttman-McCabe}, Anna and Harrison, Emma (Chaz) and Hoban, Laura and Hwang, William A. and Iannetta, Claire and Koenig, Kristen M. and Lo, Chauncey and Palone, Victoria and Pepitone, Gina and Ritzau, Margaret and Sung, Yi Hua and Thompson, Lauren and {de Leeuw}, Joshua R.},
  editor = {Math{\^o}t, Sebastiaan},
  year = 2025,
  month = jul,
  journal = {Collabra: Psychology},
  volume = {11},
  number = {1},
  pages = {140755},
  issn = {2474-7394},
  doi = {10.1525/collabra.140755},
  urldate = {2025-09-22},
  abstract = {Web-based data collection allows researchers to recruit large and diverse samples with fewer resources than lab-based studies require. Recent innovations have expanded the set of methodolgies that are possible online, but ongoing work is needed to test the suitability of web-based tools for various research paradigms. Here, we focus on webcam-based eye-tracking; we tested whether the results of five different eye-tracking experiments in the cognitive psychology literature would replicate in a webcam-based format. Specifically, we carried out five experiments by integrating two javascript-based tools: jsPsych and a modified version of Webgazer.js. In order to represent a wide range of applications of eye-tracking to cognitive psychology, we chose two psycholinguistic experiments, two memory experiments, and a decision-making experiment. These studies also varied in the type of eye-tracking display, including screens split into halves (Exps. 3 and 5) or quadrants (Exps. 2 and 4), or composed scenes with regions of interest that varied in size (Exp. 1). Outcomes were mixed. The least successful replication attempt was Exp. 1; we did not obtain a condition effect in our remote sample (1a), nor in an in-lab follow-up (1b). However, the other four experiments were more successful, replicating a blank-screen effect (Exp. 2), a novelty preference (Exp. 3), a verb bias effect (Exp. 4), and a gaze-bias effect in decision-making (Exp. 5). These results suggest that webcam-based eye-tracking can be used to detect a variety of cognitive phenomena, including those with sensitive time, although paradigms that require high spatial resolution (like Exp. 1) should be adapted to coarser quadrant or split-half displays.},
  file = {/Users/jasongeller/Zotero/storage/UUZMCEYT/James et al. - 2025 - What Paradigms Can Webcam Eye-Tracking Be Used For Attempted Replications of Five Cognitive Science.pdf}
}

@inproceedings{papoutsakiWebGazerScalableWebcam2016a,
  title = {{{WebGazer}}: {{Scalable Webcam Eye Tracking Using User Interactions}}},
  shorttitle = {{{WebGazer}}},
  booktitle = {International {{Joint Conference}} on {{Artificial Intelligence}}},
  author = {Papoutsaki, Alexandra and Sangkloy, Patsorn and Laskey, James and Daskalova, Nediyana and Huang, Jeff and Hays, James},
  year = 2016,
  month = jul,
  urldate = {2025-09-22},
  abstract = {We introduce WebGazer, an online eye tracker that uses common webcams already present in laptops and mobile devices to infer the eye-gaze locations of web visitors on a page in real time. The eye tracking model self-calibrates by watching web visitors interact with the web page and trains a mapping between features of the eye and positions on the screen. This approach aims to provide a natural experience to everyday users that is not restricted to laboratories and highly controlled user studies. WebGazer has two key components: a pupil detector that can be combined with any eye detection library, and a gaze estimator using regression analysis informed by user interactions. We perform a large remote online study and a small in-person study to evaluate WebGazer. The findings show that WebGazer can learn from user interactions and that its accuracy is sufficient for approximating the user's gaze. As part of this paper, we release the first eye tracking library that can be easily integrated in any website for real-time gaze interactions, usability studies, or web research.}
}

@article{reipsWebbasedResearchPsychology2021,
  title = {Web-Based Research in Psychology: {{A}} Review},
  shorttitle = {Web-Based Research in Psychology},
  author = {Reips, Ulf-Dietrich},
  year = 2021,
  journal = {Zeitschrift f{\"u}r Psychologie},
  volume = {229},
  number = {4},
  pages = {198--213},
  publisher = {Hogrefe Publishing},
  address = {Germany},
  issn = {2151-2604},
  doi = {10.1027/2151-2604/a000475},
  abstract = {The present article reviews web-based research in psychology. It captures principles, learnings, and trends in several types of web-based research that show similar developments related to web technology and its major shifts (e.g., appearance of search engines, browser wars, deep web, commercialization, web services, HTML5{\dots}) as well as distinct challenges. The types of web-based research discussed are web surveys and questionnaire research, web-based tests, web experiments, Mobile Experience Sampling, and non-reactive web research, including big data. A number of web-based methods are presented and discussed that turned out to become important in research methodology. These are one-item-one-screen design, seriousness check, instruction manipulation and other attention checks, multiple site entry technique, subsampling technique, warm-up technique, and web-based measurement. Pitfalls and best practices are described then, especially regarding dropout and other non-response, recruitment of participants, and interaction between technology and psychological factors. The review concludes with a discussion of important concepts that have developed over 25 years and an outlook on future developments in web-based research. (PsycInfo Database Record (c) 2025 APA, all rights reserved)},
  keywords = {Experimentation,Internet,Measurement,Methodology,Online Experiments,Psychology,Surveys},
  file = {/Users/jasongeller/Zotero/storage/2CRRGJ8F/Reips - 2021 - Web-based research in psychology A review.pdf}
}

@article{saxenaDeepLearningModels2024,
  title = {Deep Learning Models for Webcam Eye Tracking in Online Experiments},
  author = {Saxena, Shreshth and Fink, Lauren K. and Lange, Elke B.},
  year = 2024,
  month = jun,
  journal = {Behavior Research Methods},
  volume = {56},
  number = {4},
  pages = {3487--3503},
  issn = {1554-3528},
  doi = {10.3758/s13428-023-02190-6},
  urldate = {2025-09-22},
  abstract = {Eye tracking is prevalent in scientific and commercial applications. Recent computer vision and deep learning methods enable eye tracking with off-the-shelf webcams and reduce dependence on expensive, restrictive hardware. However, such deep learning methods have not yet been applied and evaluated for remote, online psychological experiments. In this study, we tackle critical challenges faced in remote eye tracking setups and systematically evaluate appearance-based deep learning methods of gaze tracking and blink detection. From their own homes and laptops, 65 participants performed a battery of eye~tracking tasks including (i) fixation, (ii) zone classification, (iii) free viewing, (iv) smooth pursuit, and (v) blink detection. Webcam recordings of the participants performing these tasks were processed offline through appearance-based models of gaze and blink detection. The task battery required different eye movements that characterized gaze and blink prediction accuracy over a comprehensive list of measures. We find the best gaze accuracy to be 2.4{$^\circ$} and precision of 0.47{$^\circ$}, which outperforms previous online eye~tracking studies and reduces the gap between laboratory-based and online eye~tracking performance. We release the experiment template, recorded data, and analysis code with the motivation to escalate affordable, accessible, and scalable eye tracking that has the potential to accelerate research in the fields of psychological science, cognitive neuroscience, user experience design, and human--computer interfaces.},
  langid = {english},
  keywords = {Blinks,Computer vision,Deep learning,Eye gaze,Eye tracking,Fixation,Free viewing,Low resolution,Online,Smooth pursuit},
  file = {/Users/jasongeller/Zotero/storage/J37YA3TU/Saxena et al. - 2024 - Deep learning models for webcam eye tracking in online experiments.pdf}
}

@article{yangWebcambasedOnlineEyetracking2021,
  title = {Webcam-Based Online Eye-Tracking for Behavioral Research},
  author = {Yang, Xiaozhi and Krajbich, Ian},
  year = 2021,
  month = nov,
  journal = {Judgment and Decision Making},
  volume = {16},
  number = {6},
  pages = {1485--1505},
  issn = {1930-2975},
  doi = {10.1017/S1930297500008512},
  urldate = {2025-09-22},
  abstract = {Experiments are increasingly moving online. This poses a major challenge forresearchers who rely on in-lab techniques such as eye-tracking. Researchers incomputer science have developed web-based eye-tracking applications (WebGazer;Papoutsaki et al., 2016) but they have yet to see them used in behavioralresearch. This is likely due to the extensive calibration and validationprocedure, inconsistent temporal resolution (Semmelmann \& Weigelt, 2018),and the challenge of integrating it into experimental software. Here, weincorporate WebGazer into a JavaScript library widely used by behavioralresearchers (jsPsych) and adjust the procedure and code to reducecalibration/validation and improve the temporal resolution (from 100--1000ms to 20--30 ms). We test this procedure with a decision-making study onAmazon MTurk, replicating previous in-lab findings on the relationship betweengaze and choice, with little degradation in spatial or temporal resolution. Thisprovides evidence that online web-based eye-tracking is feasible in behavioralresearch.},
  keywords = {attention,attentional drift diffusion model,decision-making,eye-tracking,online studies,preferences},
  file = {/Users/jasongeller/Zotero/storage/YX8XJ4ZS/Yang and Krajbich - 2021 - Webcam-based online eye-tracking for behavioral research.pdf}
}

@article{reips2021,
	title = {Web-based research in psychology: A review},
	author = {Reips, Ulf-Dietrich},
	year = {2021},
	date = {2021},
	journal = {Zeitschrift für Psychologie},
	pages = {198--213},
	volume = {229},
	number = {4},
	doi = {10.1027/2151-2604/a000475},
	note = {Place: Germany
Publisher: Hogrefe Publishing}
}

@article{Anwyl-Irvine2020,
	title = {Gorilla in our midst: An online behavioral experiment builder},
	author = {Anwyl-Irvine, Alexander L. and {Massonnié}, Jessica and Flitton, Adam and Kirkham, Natasha and Evershed, Jo K.},
	year = {2019},
	month = {04},
	date = {2019-04-23},
	journal = {Behavior Research Methods},
	pages = {388--407},
	volume = {52},
	number = {1},
	doi = {10.3758/s13428-019-01237-x},
	url = {http://dx.doi.org/10.3758/s13428-019-01237-x},
	langid = {en}
}

@article{Peirce2019,
	title = {PsychoPy2: Experiments in behavior made easy},
	author = {Peirce, Jonathan and Gray, Jeremy R. and Simpson, Sol and MacAskill, Michael and {Höchenberger}, Richard and Sogo, Hiroyuki and Kastman, Erik and {Lindeløv}, Jonas Kristoffer},
	year = {2019},
	month = {02},
	date = {2019-02},
	journal = {Behavior Research Methods},
	pages = {195--203},
	volume = {51},
	number = {1},
	doi = {10.3758/s13428-018-01193-y},
	url = {http://dx.doi.org/10.3758/s13428-018-01193-y},
	langid = {en}
}

@article{de2015,
	title = {jsPsych: A JavaScript library for creating behavioral experiments in a Web browser},
	author = {de Leeuw, Joshua R.},
	year = {2014},
	month = {03},
	date = {2014-03-28},
	journal = {Behavior Research Methods},
	pages = {1--12},
	volume = {47},
	number = {1},
	doi = {10.3758/s13428-014-0458-y},
	url = {http://dx.doi.org/10.3758/s13428-014-0458-y},
	langid = {en}
}

@article{zehr2022,
	title = {PennController for Internet Based Experiments (IBEX)},
	author = {Zehr, {Jérémy} and Schwarz, Florian},
	year = {2022},
	month = {08},
	date = {2022-08-30},
	journal = {Open Science Framework},
	doi = {10.17605/OSF.IO/MD832},
	url = {https://osf.io/md832/}
}

@article{Kaduk2024,
	title = {Webcam eye tracking close to laboratory standards: Comparing a new webcam-based system and the EyeLink 1000},
	author = {Kaduk, Tobiasz and Goeke, Caspar and Finger, Holger and {König}, Peter},
	year = {2023},
	month = {10},
	date = {2023-10-11},
	journal = {Behavior Research Methods},
	pages = {5002--5022},
	volume = {56},
	number = {5},
	doi = {10.3758/s13428-023-02237-8},
	url = {http://dx.doi.org/10.3758/s13428-023-02237-8},
	langid = {en}
}

@article{slim2024,
	title = {Webcams as Windows to the Mind? A Direct Comparison Between In-Lab and Web-Based Eye-Tracking Methods},
	author = {Slim, Mieke Sarah and Kandel, Margaret and Yacovone, Anthony and Snedeker, Jesse},
	year = {2024},
	month = {11},
	date = {2024-11-22},
	journal = {Open Mind},
	pages = {1369--1424},
	volume = {8},
	doi = {10.1162/opmi_a_00171},
	url = {https://doi.org/10.1162/opmi_a_00171},
	note = {Citation Key: slimWebcamsWindowsMind2024}
}

@article{bogdan2024,
	title = {Investigating the suitability of online eye tracking for psychological research: Evidence from comparisons with in-person data using emotion{\textendash}attention interaction tasks},
	author = {Bogdan, Paul C. and Dolcos, Sanda and Buetti, Simona and Lleras, Alejandro and Dolcos, Florin},
	year = {2024},
	month = {03},
	date = {2024-03-01},
	journal = {Behavior Research Methods},
	pages = {2213--2226},
	volume = {56},
	number = {3},
	doi = {10.3758/s13428-023-02143-z},
	url = {https://doi.org/10.3758/s13428-023-02143-z},
	langid = {en}
}

@article{bramlett2024,
	title = {The art of wrangling},
	author = {Bramlett, Adam A. and Wiener, Seth},
	year = {2024},
	date = {2024},
	journal = {Linguistic Approaches to Bilingualism},
	doi = {https://doi.org/10.1075/lab.23071.bra},
	url = {https://www.jbe-platform.com/content/journals/10.1075/lab.23071.bra},
	note = {Publisher: John Benjamins
Citation Key: bramlettArtWrangling2024}
}

@article{özsoy2023,
	title = {Turkish-German heritage speakers' predictive use of case: webcam-based vs. in-lab eye-tracking},
	author = {{Özsoy}, Onur and {Çiçek}, {Büsra} and {Özal}, Zeynep and Gagarina, Natalia and Sekerina, Irina A.},
	year = {2023},
	month = {07},
	date = {2023-07-19},
	journal = {Frontiers in Psychology},
	pages = {1155585},
	volume = {14},
	doi = {10.3389/fpsyg.2023.1155585},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10395335/},
	note = {PMID: 37539000
PMCID: PMC10395335}
}

@article{prystauka2024,
	title = {Online eye tracking and real-time sentence processing: On opportunities and efficacy for capturing psycholinguistic effects of different magnitudes and diversity},
	author = {Prystauka, Yanina and Altmann, Gerry T. M. and Rothman, Jason},
	year = {2024},
	month = {06},
	date = {2024-06-01},
	journal = {Behavior Research Methods},
	pages = {3504--3522},
	volume = {56},
	number = {4},
	doi = {10.3758/s13428-023-02176-4},
	url = {https://doi.org/10.3758/s13428-023-02176-4},
	note = {Citation Key: prystaukaOnlineEyeTracking2024},
	langid = {en}
}

@article{slim2023,
	title = {Moving visual world experiments online? A web-based replication of Dijkgraaf, Hartsuiker, and Duyck (2017) using PCIbex and WebGazer.js},
	author = {Slim, Mieke Sarah and Hartsuiker, Robert J.},
	year = {2023},
	month = {10},
	date = {2023-10-01},
	journal = {Behavior Research Methods},
	pages = {3786--3804},
	volume = {55},
	number = {7},
	doi = {10.3758/s13428-022-01989-z},
	url = {https://doi.org/10.3758/s13428-022-01989-z},
	note = {Citation Key: slimMovingVisualWorld2023},
	langid = {en}
}

@article{vandercruyssen2023,
	title = {The validation of online webcam-based eye-tracking: The replication of the cascade effect, the novelty preference, and the visual world paradigm},
	author = {Van der Cruyssen, Ine and Ben-Shakhar, Gershon and Pertzov, Yoni and Guy, Nitzan and Cabooter, Quinn and Gunschera, Lukas J. and Verschuere, Bruno},
	year = {2023},
	month = {08},
	date = {2023-08-30},
	journal = {Behavior Research Methods},
	doi = {10.3758/s13428-023-02221-2},
	url = {https://doi.org/10.3758/s13428-023-02221-2},
	note = {Citation Key: vandercruyssenValidationOnlineWebcambased2023},
	langid = {en}
}

@article{vos2022,
	title = {Comparing infrared and webcam eye tracking in the Visual World Paradigm},
	author = {Vos, Myrte and Minor, Serge and Ramchand, Gillian Catriona},
	year = {2022},
	month = {08},
	date = {2022-08-15},
	journal = {Glossa Psycholinguistics},
	volume = {1},
	number = {1},
	doi = {10.5070/G6011131},
	url = {https://escholarship.org/uc/item/3r28x18w},
	note = {Citation Key: vosComparingInfraredWebcam2022},
	langid = {en}
}

@article{patterson2025,
	title = {Methodological recommendations for webcam-based eye tracking: A scoping review},
	author = {Patterson, Allie Spencer and Nicklin, Christopher and Vitta, Joseph P.},
	year = {2025},
	month = {12},
	date = {2025-12},
	journal = {Research Methods in Applied Linguistics},
	pages = {100244},
	volume = {4},
	number = {3},
	doi = {10.1016/j.rmal.2025.100244},
	url = {http://dx.doi.org/10.1016/j.rmal.2025.100244},
	langid = {en}
}

@article{semmelmann2018,
	title = {Online webcam-based eye tracking in cognitive science: A first look},
	author = {Semmelmann, Kilian and Weigelt, Sarah},
	year = {2018},
	month = {04},
	date = {2018-04-01},
	journal = {Behavior Research Methods},
	pages = {451--465},
	volume = {50},
	number = {2},
	doi = {10.3758/s13428-017-0913-7},
	url = {https://doi.org/10.3758/s13428-017-0913-7},
	note = {Citation Key: semmelmannOnlineWebcambasedEye2018},
	langid = {en}
}

@article{cooper1974,
	title = {The control of eye fixation by the meaning of spoken language: A new methodology for the real-time investigation of speech perception, memory, and language processing},
	author = {Cooper, Roger M.},
	year = {1974},
	month = {01},
	date = {1974-01},
	journal = {Cognitive Psychology},
	pages = {84--107},
	volume = {6},
	number = {1},
	doi = {10.1016/0010-0285(74)90005-X},
	url = {https://www.sciencedirect.com/science/article/pii/001002857490005X},
	note = {Publisher: Academic Press
Citation Key: cooperControlEyeFixation1974}
}

@article{tanenhaus1995,
	title = {Integration of visual and linguistic information in spoken language comprehension.},
	author = {Tanenhaus, M K and Spivey-Knowlton, M J and Eberhard, K M and Sedivy, J C},
	year = {1995},
	month = {06},
	date = {1995-06},
	journal = {Science (New York, N.Y.)},
	pages = {1632--4},
	volume = {268},
	number = {5217},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/7777863},
	note = {Citation Key: tanenhausIntegrationVisualLinguistic1995}
}

@article{huettig2011,
	title = {Using the visual world paradigm to study language processing: a review and critical evaluation},
	author = {Huettig, Falk and Rommers, Joost and Meyer, Antje S.},
	year = {2011},
	month = {06},
	date = {2011-06},
	journal = {Acta Psychologica},
	pages = {151--171},
	volume = {137},
	number = {2},
	doi = {10.1016/j.actpsy.2010.11.003},
	note = {PMID: 21288498
Citation Key: huettigUsingVisualWorld2011a},
	langid = {eng}
}

@article{colby2023,
	title = {Efficiency of spoken word recognition slows across the adult lifespan},
	author = {Colby, Sarah E. and McMurray, Bob},
	year = {2023},
	month = {11},
	date = {2023-11},
	journal = {Cognition},
	pages = {105588},
	volume = {240},
	doi = {10.1016/j.cognition.2023.105588},
	note = {PMID: 37586157
PMCID: PMC10530619},
	langid = {eng}
}

@article{mcmurray2010,
	title = {Individual differences in online spoken word recognition: Implications for SLI},
	author = {McMurray, Bob and Samelson, Vicki M. and Lee, Sung Hee and Tomblin, J. Bruce},
	year = {2010},
	date = {2010},
	journal = {Cognitive Psychology},
	pages = {1--39},
	volume = {60},
	number = {1},
	doi = {10.1016/j.cogpsych.2009.06.003},
	note = {Place: Netherlands
Publisher: Elsevier Science
Citation Key: mcmurrayIndividualDifferencesOnline2010a}
}
